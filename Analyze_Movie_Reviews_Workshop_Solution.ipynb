{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assembly Workshop 2 - Analyze Movie Reviews",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVyLscxqj2R"
      },
      "source": [
        "#**Assembly Workshop -- Analyze Movie Reviews**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ7V6g2BxDAT"
      },
      "source": [
        "##**Part One - Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CpLq_a9xTsS"
      },
      "source": [
        "##**Step 1: Import the required libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryLJLUNmxc_Z"
      },
      "source": [
        "Import the numpy and tensorflow libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hOu6axEx4nq"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZa5zZ1kxvgK"
      },
      "source": [
        "##**Step 2: Import the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNAfBz4Qx5fy"
      },
      "source": [
        "Our dataset comes from tf.keras.datasets.imdb, and has two functions: `get_word_index()` and `load_data()`.\r\n",
        "\r\n",
        "Let's begin by loading our data into our testing and training sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otFjft9xxcdJ"
      },
      "source": [
        "data = tf.keras.datasets.imdb\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = data.load_data()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vokaoPAjyn39"
      },
      "source": [
        "##**Step 3: Importing our dictionary (map)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVYg3KjgywB-"
      },
      "source": [
        "`get_word_index() ` returns a dictionary of each word mapping to a certain integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytll4OPsxSzz"
      },
      "source": [
        "word_to_int = data.get_word_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjRfo-bDzIl4"
      },
      "source": [
        "##**Step 4: Writing a function to decode the text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsM9N9XzQj6"
      },
      "source": [
        "First, `index_from` is set to 3. So we need to 'shift' our dictionary by 3 ints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACudBgU9zPWz"
      },
      "source": [
        "word_to_int = {k : (v+3) for k,v in word_to_int.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbEG2gd6j0ko"
      },
      "source": [
        "Moreover, we need to map some keys into ints for processing. Remember that when we call the `load_data()` function, the 'start' character is 1, out-of-vocabulary 'character' is 2, and 0 is for padding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kepSjBOij0-e"
      },
      "source": [
        "word_to_int['<PAD>'] = 0\r\n",
        "word_to_int['<START>'] = 1\r\n",
        "word_to_int['<UNK>'] = 2\r\n",
        "word_to_int['<UNUSED>'] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYMJ6DDbkQiO"
      },
      "source": [
        "Next, we need to map each int to a word in order to decode the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Xtn7FEkQ26"
      },
      "source": [
        "int_to_word = dict([(value, key) for (key, value) in word_to_int.items()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV0aimWxkZlf"
      },
      "source": [
        "Finally, we can write our decode function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzE1nSzDkaDe"
      },
      "source": [
        "def decode_text(text):\r\n",
        "  return ' '.join([int_to_word.get(i, '?') for i in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CHU5J9TzbDJ"
      },
      "source": [
        "##**Step 5: Padding our data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZRuwbUEz7Mh"
      },
      "source": [
        "Its important that all our data that's being fed into our model is of the same length, so we'll be padding data that is shorter than our specified length and trimming the data that is longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IACSwotSzb3f"
      },
      "source": [
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(sequences = x_train,\r\n",
        "                                                        maxlen = 300,\r\n",
        "                                                        padding = 'post',\r\n",
        "                                                        value = word_to_int['<PAD>'])\r\n",
        "\r\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(sequences = x_test,\r\n",
        "                                                        maxlen = 300,\r\n",
        "                                                        padding = 'post',\r\n",
        "                                                        value = word_to_int['<PAD>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbVabnVt0l6e"
      },
      "source": [
        "#**Part 2: Building the NN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv8w-77N0qg3"
      },
      "source": [
        "##**Step 1: Initialise the NN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXPUuYYT3JgP"
      },
      "source": [
        "To initialise the NN, we need to find the Sequential class from the tensorflow keras library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSdFXPc10uFN"
      },
      "source": [
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXhg5UT60ubt"
      },
      "source": [
        "##**Step 2: Adding the Embedding layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upctF73l3UXP"
      },
      "source": [
        "The embedding layer must be the first layer of our neural network. `input_dim` is going to be the number of words in our dictionary (check `load_data()`), and `output_dim` is going to be the dimensions of our vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cjgPcVW3HpN"
      },
      "source": [
        "model.add(tf.keras.layers.Embedding(input_dim = 88500, output_dim = 16))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbpKmfnV3fVi"
      },
      "source": [
        "##**Step 3: Adding the Global Average Pooling 1D layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR0uUZzR3iu0"
      },
      "source": [
        "This layer simply reduces the dimensions in our vectors to make it computationally less expensive to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-3BnVW3iUA"
      },
      "source": [
        "model.add(tf.keras.layers.GlobalAveragePooling1D())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihK0JIYa3ueM"
      },
      "source": [
        "##**Step 4: Adding the Dense layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQNHbrM534gh"
      },
      "source": [
        "Our first dense layer will have 32 neurons and will use the 'relu' activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUnujAZo35DV"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units = 32, activation = 'relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2THz6uRm9bQ"
      },
      "source": [
        "Our second dense layer is also our output neuron, so it will only have 1 neuron and a 'sigmoid' activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Cfq0ujwBtd"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp7C6_YF4UTB"
      },
      "source": [
        "#**Part 3: Training our model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBfkvR1K4YhN"
      },
      "source": [
        "##**Step 1: Compiling our model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmYBwjxG4f-H"
      },
      "source": [
        "We'll compile the network using the adam optimizer and the binary crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL_RI-qK4Y09"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niWQyRv_43a4"
      },
      "source": [
        "##**Step 2: Training the NN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKCoqpnu43K1"
      },
      "source": [
        "Using the .fit function, we will train the NN using a batch size of 1000 through 50 epochs. Moreover, we'll use 20% of our data as a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_U8VKh15DJF"
      },
      "source": [
        "model.fit(x_train, y_train, epochs = 50, batch_size = 1000, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIilHOXm5MIl"
      },
      "source": [
        "#**Part 4: Testing the NN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ir52B_vnXXL"
      },
      "source": [
        "##**Step 1: Encoding our review**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQE20OxA5P_x"
      },
      "source": [
        "We need to create a function to encode the review so that our model can read and process the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n1HRB6XBASt"
      },
      "source": [
        "def encode_review(text):\r\n",
        "    encoded = []\r\n",
        "    encoded.append(1)\r\n",
        "    \r\n",
        "    for word in text:\r\n",
        "        if word.lower() in word_to_int:\r\n",
        "            encoded.append(word_to_int[word.lower()])\r\n",
        "        else:\r\n",
        "            encoded.append(2)\r\n",
        "    \r\n",
        "    return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX5xZ5Lp_7dn"
      },
      "source": [
        "##**Step 2: Reading and classifying the review**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjgNE6rinwQ2"
      },
      "source": [
        "Now, we can enter the file name and pass the data into our model to predict whether or not the reviewer liked or disliked the movie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKFPKZSB_9Ah"
      },
      "source": [
        "filename = 'rev.txt'\r\n",
        "\r\n",
        "#open text file and predict output  \r\n",
        "with open(filename) as file:\r\n",
        "    for line in file.readlines():\r\n",
        "        #remove all unneccessary characters from the text file\r\n",
        "        newLine = line.replace(',',\"\").replace('-', '').replace('.',\"\").replace('\\\"',\"\").replace('\\'',\"\").replace(':',\"\").replace('(',\"\").replace(')',\"\").strip().split(' ')\r\n",
        "        encode = encode_review(newLine)\r\n",
        "        \r\n",
        "        encode = tf.keras.preprocessing.sequence.pad_sequences([encode], maxlen = 300, padding='post',value = word_to_int['<PAD>'])\r\n",
        "        \r\n",
        "        predict = model.predict(encode)\r\n",
        "        print(line)\r\n",
        "        print(encode)\r\n",
        "        print(predict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}